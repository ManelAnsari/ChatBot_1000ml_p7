{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chatbot_Attempt3.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNEB4OQDvnHW9ixJf65AUT5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"BZIY7f8cUOKX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1592937532898,"user_tz":240,"elapsed":55148,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}},"outputId":"a1af63b1-87be-413e-e56e-4e87e8dc00c5"},"source":["# We'll first need the data from my drive.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3GZ4dGetd9ec","colab_type":"text"},"source":["# Process\n","\n","1. Create Intents\n","2. "]},{"cell_type":"markdown","metadata":{"id":"onA0_omtU7wL","colab_type":"text"},"source":["# Packages"]},{"cell_type":"code","metadata":{"id":"YrEWH-u1Uw6C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1592937573551,"user_tz":240,"elapsed":6452,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}},"outputId":"a1f47eac-b491-43b1-893e-8c731d55fe36"},"source":["path='/content/drive/My Drive/1000ml/Project 7 - Chatbot/Data/'\n","\n","# We'll need the following nltk packages\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# We'll use lemmas instead of stems.\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# We will tag the parts of the speech \n","from nltk.tag import pos_tag\n","\n","# Get the stop words\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# We'll need json to open a file of intents\n","import json\n","\n","# We will need to save the model\n","import pickle\n","\n","# Some helper libraries\n","import numpy as np\n","import pandas as pd\n","import random\n","import re\n","\n","# We'll need these to actually create a machine learning model.\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","from keras.optimizers import SGD, Adagrad, Adam, Adadelta\n","\n","# We may need to try out a grid search\n","from sklearn.model_selection import GridSearchCV\n","\n","# We'll need this for the in-code tagging of words\n","import spacy\n","from spacy import displacy\n","from collections import Counter\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","from spacy.matcher import Matcher"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LVFuEfFyU63u","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592925631313,"user_tz":240,"elapsed":1067,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","from spacy.matcher import Matcher\n","m_tool = Matcher(nlp.vocab)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"uraxUt0gZKfs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592852543436,"user_tz":240,"elapsed":346,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["p1 = [{'LOWER': 'from'}, {'ENT_TYPE':'GPE'}]\n","p2 = [{'LOWER': 'to'}, {'ENT_TYPE': 'GPE'}]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNz7HgM2ZRI8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592852543540,"user_tz":240,"elapsed":188,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["m_tool.add('OrDest', None, p1, p2)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgYj25tMZRqG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592852544112,"user_tz":240,"elapsed":418,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["sentence = nlp(u'I want to book a flight from Toronto to Vancouver')"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITEYAHX5ZTn2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592852544115,"user_tz":240,"elapsed":210,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}},"outputId":"0f4757cc-10c8-4588-c7ef-93ce7fd70bb2"},"source":["phrase_matches = m_tool(sentence)\n","print(phrase_matches )"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[(14941694479039199002, 6, 8), (14941694479039199002, 8, 10)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tYG74i9dZVt9","colab_type":"text"},"source":["# Copy Paste from Article"]},{"cell_type":"code","metadata":{"id":"oZWMkmf3ikQY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937580557,"user_tz":240,"elapsed":10012,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["# first we'll need the pairs of conversation:\n","data_file1 = open(f'{path}frames.json').read()\n","travel_txt = json.loads(data_file1)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIJyEVYSjgBR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937580566,"user_tz":240,"elapsed":9474,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["user_txt=[]\n","robo_txt=[]\n","for convo in travel_txt:\n","  if len(convo['turns'])%2==0:\n","    for i in range(0, len(convo['turns']), 2):\n","      user_txt.append(convo['turns'][i]['text'])\n","      robo_txt.append(convo['turns'][i+1]['text'])\n","  else:\n","    for i in range(0, len(convo['turns'])-1, 2):\n","      user_txt.append(convo['turns'][i]['text'])\n","      robo_txt.append(convo['turns'][i+1]['text'])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SbjkW6xibXf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937580569,"user_tz":240,"elapsed":8961,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["# Cleaning Data\n","lines = [re.sub(r\"\\[\\w+\\]\",'hi',line) for line in user_txt]\n","lines = [\" \".join(re.findall(r\"\\w+\",line)) for line in lines]\n","lines2 = [re.sub(r\"\\[\\w+\\]\",'',line) for line in robo_txt]\n","lines2 = [\" \".join(re.findall(r\"\\w+\",line)) for line in lines2]\n","\n","# grouping lines by response pair\n","pairs = list(zip(lines,lines2))\n","random.shuffle(pairs)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qpwn1uDonbcH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937580571,"user_tz":240,"elapsed":3485,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["def create_tokens(pairs, start, end):\n","  input_docs = []\n","  target_docs = []\n","  input_tokens = set()\n","  target_tokens = set()\n","\n","  for line in pairs[start:end]:\n","    input_doc, target_doc = line[0], line[1]\n","    # Appending each input sentence to input_docs\n","    input_docs.append(input_doc)\n","    # Splitting words from punctuation  \n","    target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n","    # Redefine target_doc below and append it to target_docs\n","    target_doc = '<START> ' + target_doc + ' <END>'\n","    target_docs.append(target_doc)\n","    \n","    # Now we split up each sentence into words and add each unique word to our vocabulary set\n","    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n","      if token not in input_tokens:\n","        input_tokens.add(token)\n","    for token in target_doc.split():\n","      if token not in target_tokens:\n","        target_tokens.add(token)\n","\n","  input_tokens = sorted(list(input_tokens))\n","  target_tokens = sorted(list(target_tokens))\n","  num_encoder_tokens = len(input_tokens)\n","  num_decoder_tokens = len(target_tokens)\n","  return (input_docs, input_tokens, target_docs, target_tokens, num_encoder_tokens, num_decoder_tokens)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SzUOMh8v4ZSV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937591362,"user_tz":240,"elapsed":717,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["# The reason for this is we are running out of RAM. See if I can't chunk up the info, and combine it at the end.\n","input_docs1, input_tokens1, target_docs1, target_tokens1, num_encoder_tokens1, num_decoder_tokens1 = create_tokens(pairs,0,2500)\n","# input_docs2, input_tokens2, target_docs2, target_tokens2, num_encoder_tokens2, num_decoder_tokens2 = create_tokens(pairs,2500,5000)\n","# input_docs3, input_tokens3, target_docs3, target_tokens3, num_encoder_tokens3, num_decoder_tokens3 = create_tokens(pairs,5000,7500)\n","# input_docs4, input_tokens4, target_docs4, target_tokens4, num_encoder_tokens4, num_decoder_tokens4 = create_tokens(pairs,7500,len(pairs))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xs5cGzYJpCJW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937593612,"user_tz":240,"elapsed":763,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["# Below we'll need two versions of the same dictionary for different purposes. Just with the keys and positions reversed.\n","def feature_dicts(input_tokens, target_tokens):\n","  input_features_dict = dict([(token, i) for i, token in enumerate(input_tokens)])\n","\n","  target_features_dict = dict([(token, i) for i, token in enumerate(target_tokens)])\n","\n","  reverse_input_features_dict = dict((i, token) for token, i in input_features_dict.items())\n","\n","  reverse_target_features_dict = dict((i, token) for token, i in target_features_dict.items())\n","\n","  return (input_features_dict, target_features_dict, reverse_input_features_dict, reverse_target_features_dict)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"A8K04JkK6dRF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937593921,"user_tz":240,"elapsed":431,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["input_features_dict1, target_features_dict1, reverse_input_features_dict1, reverse_target_features_dict1 = feature_dicts(input_tokens1, target_tokens1)\n","# input_features_dict2, target_features_dict2, reverse_input_features_dict2, reverse_target_features_dict2 = feature_dicts(input_tokens2, target_tokens2)\n","# input_features_dict3, target_features_dict3, reverse_input_features_dict3, reverse_target_features_dict3 = feature_dicts(input_tokens3, target_tokens3)\n","# input_features_dict4, target_features_dict4, reverse_input_features_dict4, reverse_target_features_dict4 = feature_dicts(input_tokens4, target_tokens4)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvViAygrqLhy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937595265,"user_tz":240,"elapsed":611,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["def create_bow_data(input_docs, target_docs, input_features_dict, target_features_dict, num_encoder_tokens, num_decoder_tokens):\n","  #Maximum length of sentences in input and target documents\n","  max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n","  max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n","\n","  # This is basically making a 3 dimensional bag of words\n","  encoder_input_data = np.zeros((len(input_docs), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n","  decoder_input_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n","  decoder_target_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n","\n","  for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n","      for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n","          #Assign 1. for the current line, timestep, & word in encoder_input_data\n","          encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n","      # the second bit here is to incorporate \"teaching forcing\". A method of Recursive learning which helps a model learn, by comparing with the answer at each step.\n","      for timestep, token in enumerate(target_doc.split()):\n","          decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n","          if timestep > 0:\n","              decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1.\n","  return (encoder_input_data, decoder_input_data, decoder_target_data)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kCH2VHu14bA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937597065,"user_tz":240,"elapsed":1223,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["def save_large_arrays(input_docs, target_docs, input_features_dict, target_features_dict, num_encoder_tokens, num_decoder_tokens):\n","  with open('encoder_input1.npy', 'wb') as f:\n","    np.save(f, create_bow_data(input_docs1, target_docs1, input_features_dict1, target_features_dict1, num_encoder_tokens1, num_decoder_tokens1)[0], allow_pickle=True)\n","  with open('decoder_input1.npy', 'wb') as f:\n","    np.save(f, create_bow_data(input_docs1, target_docs1, input_features_dict1, target_features_dict1, num_encoder_tokens1, num_decoder_tokens1)[1], allow_pickle=True)\n","  with open('decoder_target1.npy', 'wb') as f:\n","    np.save(f, create_bow_data(input_docs1, target_docs1, input_features_dict1, target_features_dict1, num_encoder_tokens1, num_decoder_tokens1)[2], allow_pickle=True)  "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIAUTwpc76Hy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937604592,"user_tz":240,"elapsed":7315,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["encoder_input1, decoder_input1, decoder_target1 = create_bow_data(input_docs1, target_docs1, input_features_dict1, target_features_dict1, num_encoder_tokens1, num_decoder_tokens1)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5KHQub2_3bj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592937605381,"user_tz":240,"elapsed":6590,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}}},"source":["from tensorflow import keras\n","from keras.layers import Input, LSTM, Dense\n","from keras.models import Model\n","#Dimensionality\n","dimensionality = 256\n","#The batch size and number of epochs\n","batch_size = 10\n","epochs = 100\n","#Encoder\n","encoder_inputs = Input(shape=(None, num_encoder_tokens1))\n","encoder_lstm = LSTM(dimensionality, return_state=True)\n","encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n","encoder_states = [state_hidden, state_cell]\n","#Decoder\n","decoder_inputs = Input(shape=(None, num_decoder_tokens1))\n","decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n","decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens1, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIo4s9GvZhJH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"error","timestamp":1592953310682,"user_tz":240,"elapsed":3640636,"user":{"displayName":"Karl Davidson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9w6TlOQK5_j4sBkaAza6zpbaYA3qM6SUTrnVXYA=s64","userId":"10038756410563164123"}},"outputId":"3e50f5af-a596-4f92-afac-20feb5c77df3"},"source":["#Model\n","training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","#Compiling \n","training_model.compile(optimizer='Adagrad', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n","#Training\n","training_model.fit([encoder_input1, decoder_input1], decoder_target1, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n","training_model.save('training_model.h5')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Train on 2000 samples, validate on 500 samples\n","Epoch 1/100\n","2000/2000 [==============================] - 506s 253ms/step - loss: 0.2914 - accuracy: 0.0797 - val_loss: 0.5452 - val_accuracy: 0.0404\n","Epoch 2/100\n","2000/2000 [==============================] - 509s 254ms/step - loss: 0.2269 - accuracy: 0.0934 - val_loss: 0.5491 - val_accuracy: 0.0405\n","Epoch 3/100\n","2000/2000 [==============================] - 549s 274ms/step - loss: 0.2036 - accuracy: 0.0983 - val_loss: 0.5550 - val_accuracy: 0.0407\n","Epoch 4/100\n","2000/2000 [==============================] - 575s 288ms/step - loss: 0.1885 - accuracy: 0.1012 - val_loss: 0.5612 - val_accuracy: 0.0395\n","Epoch 5/100\n","2000/2000 [==============================] - 556s 278ms/step - loss: 0.1769 - accuracy: 0.1027 - val_loss: 0.5646 - val_accuracy: 0.0396\n","Epoch 6/100\n","2000/2000 [==============================] - 559s 279ms/step - loss: 0.1673 - accuracy: 0.1047 - val_loss: 0.5683 - val_accuracy: 0.0393\n","Epoch 7/100\n","1400/2000 [====================>.........] - ETA: 2:43 - loss: 0.1591 - accuracy: 0.1056"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-22f889e13fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adagrad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'temporal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"zw9tXZP6Mv-O","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}